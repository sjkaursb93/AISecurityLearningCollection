# Reduce Risk and Attack Surface

This folder contains resources, strategies, and best practices for minimizing risk and reducing the attack surface in AI and cloud environments. 

## What are the risks

To mitigate the risks, first understand the risks. Use these resources to deepen your knowledge, identify potential vulnerabilities, and apply best practices for secure and responsible AI implementation.


- [Microsoft AI Bug Bar](https://www.microsoft.com/en-US/msrc/aibugbar?msockid=2374e47992096e7a2adef0c193de6fb5): The AI Bug Bar is a Microsoft resource that provides a comprehensive checklist and guidance for identifying, categorizing, and mitigating security vulnerabilities in AI systems and models.

- [Safe.ai AI Risk](https://safe.ai/ai-risk): Safe.ai provides an overview of key risks associated with AI systems, including safety, security, and ethical concerns, along with strategies for risk reduction and responsible AI deployment.

- [AI Incident Database](https://incidentdatabase.ai/): The AI Incident Database is a collection of documented failures, vulnerabilities, and incidents involving AI systems, helping organizations learn from real-world cases to improve risk management and security.

## Mitigation and Risk Reduction

- [NCSC Guidelines for Secure AI System Development](https://www.ncsc.gov.uk/files/Guidelines-for-secure-AI-system-development.pdf): This document from the UK National Cyber Security Centre provides comprehensive guidelines and best practices for building secure AI systems, covering risk management, threat modeling, secure design, and operational controls.



### Microsoft Guidance


- [Microsoft Shared Responsibility for AI](https://learn.microsoft.com/en-us/azure/security/fundamentals/shared-responsibility-ai): This resource explains the shared responsibility model for AI security in Azure, clarifying the roles and obligations of both cloud providers and customers to ensure secure and compliant AI deployments.

- [Best Practices to Architect Secure Generative AI Applications](https://techcommunity.microsoft.com/blog/microsoft-security-blog/best-practices-to-architect-secure-generative-ai-applications/4116661): This Microsoft Security Blog post provides actionable guidance and best practices for designing, building, and deploying secure generative AI applications. It covers threat modeling, data protection, access control, monitoring, and compliance strategies to help organizations safeguard their AI solutions.

- [Microsoft's Open Automation Framework for Red Teaming Generative AI](https://www.microsoft.com/en-us/security/blog/2024/02/22/announcing-microsofts-open-automation-framework-to-red-team-generative-ai-systems/): This announcement introduces Microsoft's open-source framework for automating red team testing of generative AI systems, enabling organizations to proactively identify vulnerabilities and strengthen the security of their AI applications.

- [Defender for Cloud AI Threat Protection](https://learn.microsoft.com/en-us/azure/defender-for-cloud/ai-threat-protection): Microsoft Defender for Cloud AI Threat Protection provides advanced threat detection and security monitoring for AI workloads in Azure. It helps organizations identify, investigate, and respond to threats targeting AI models, data, and infrastructure, enhancing the overall security posture of cloud-based AI solutions.

- [Security Plan for LLM Application](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/mlops-in-openai/security/security-plan-llm-application): This Microsoft guide provides a comprehensive security plan for large language model (LLM) applications, covering risk assessment, threat modeling, secure development, and operational best practices for generative AI systems.

### Azure AI Foundry


- [Azure OpenAI Content Filters](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/content-filters): This guide explains how to use content filters in Azure OpenAI to detect and block harmful, unsafe, or sensitive outputs, helping organizations enforce responsible AI usage and compliance.

- [Azure OpenAI Blocklists](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/use-blocklists?tabs=foundry): This documentation describes how to implement blocklists in Azure OpenAI to prevent specific words, phrases, or patterns from appearing in model outputs, supporting compliance and safety requirements.

- [Azure OpenAI Risks and Safety Monitor](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/risks-safety-monitor): This guide details how to monitor and manage risks and safety in Azure OpenAI, including tools and techniques for tracking model behavior, detecting unsafe outputs, and ensuring responsible AI operations.


- [Azure Content Safety Jailbreak Analysis](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-jailbreak?pivots=programming-language-foundry-portal#analyze-attacks): This quickstart demonstrates how to analyze and detect jailbreak and prompt injection attacks in AI systems using Azure Content Safety, helping organizations identify and mitigate adversarial threats to their models.

- [Azure AI Announces Prompt Shields](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/azure-ai-announces-prompt-shields-for-jailbreak-and-indirect-prompt-injection-at/4099140): This blog post introduces Prompt Shields, a new Azure AI feature designed to detect and defend against jailbreak and indirect prompt injection attacks, enhancing the security and reliability of generative AI applications.

- [Azure Content Safety Documentation](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/): For all other content safety features, refer to this link for comprehensive documentation and guidance on protecting users and data in AI applications.


---

## Additional Microsoft Security Resources



